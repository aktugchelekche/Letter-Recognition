{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINAL_IMAGE_PREDICTION_VALIDATOR_LETTER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAyrW9h171zE",
        "outputId": "26182d4e-ecbf-490c-9ad3-d1ff16916c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r0% [1 InRelease gpgv 1,581 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,297 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [29.8 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,047 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,298 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [12.2 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,013 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [22.8 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,866 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,521 kB]\n",
            "Fetched 12.4 MB in 5s (2,681 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.0.3'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Postgres driver that will allow Spark to interact with Postgres.\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfCTP1JE8BfP",
        "outputId": "543de1a7-f4b3-43ea-96a6-33118c546ed4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-25 20:11:53--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1002883 (979K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.16.jar’\n",
            "\n",
            "postgresql-42.2.16. 100%[===================>] 979.38K  6.13MB/s    in 0.2s    \n",
            "\n",
            "2022-06-25 20:11:54 (6.13 MB/s) - ‘postgresql-42.2.16.jar’ saved [1002883/1002883]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"prediction\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
      ],
      "metadata": {
        "id": "3H3qAWi08EhE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import relevant libraries\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab.patches import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "from tensorflow.keras.models import load_model\n",
        "import base64\n",
        "from imageio import imread\n",
        "import io\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "SSB76niS8I5a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PhLpvvG8K7s",
        "outputId": "feb4ad13-f369-4e99-d315-238ab1e813ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod_data_file_Path = \"/content/drive/MyDrive/Generated_Models/\"\n",
        "mod_data_input_file = \"character_recog_model_revision_14.h5\"\n",
        "mod_to_process = mod_data_file_Path + mod_data_input_file\n",
        "print(mod_to_process)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjdspN-s8RSC",
        "outputId": "fabcdb20-b1ad-4b68-fceb-2861316c24a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Generated_Models/character_recog_model_revision_14.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained model\n",
        "trained_model = load_model(mod_to_process)"
      ],
      "metadata": {
        "id": "O2qH0itV8Szf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile trained model\n",
        "trained_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "JwZu99Lg8VYm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a refence array used for determine model prediction letter\n",
        "\n",
        "words = {0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',11:'L',12:'M',13:'N',14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',20:'U',21:'V',22:'W',23:'X', 24:'Y',25:'Z'}"
      ],
      "metadata": {
        "id": "fLjRc9vX8X9A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict letter characters using the trained model\n",
        "xx = []\n",
        "def predict_letter_from_word(in_read_image):\n",
        "    # Read the image using cv2\n",
        "    image = in_read_image\n",
        "\n",
        "    # make copy of original image, copy will be used to change colors\n",
        "    image_copy = image.copy()\n",
        "\n",
        "    # convert image to RGB using cvtColor\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize image to 400 x 400 size\n",
        "    image = cv2.resize(image, (400,400))\n",
        "\n",
        "    # Add blur to image and greyscale (need to greyscale as cv.threshold needs greyscale images)\n",
        "    image_copy = cv2.GaussianBlur(image_copy, (7,7), 0)\n",
        "    grey_image = cv2.cvtColor(image_copy, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "    # Separate object from background pixels using thresholding\n",
        "    # https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html\n",
        "    _, img_thresh = cv2.threshold(grey_image, 100, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "    # Resize and reshape image to fit trained_model requirements\n",
        "    final_image = cv2.resize(img_thresh, (28,28))\n",
        "    # xx.append(final_image)\n",
        "    # print(final_image)\n",
        "    final_image = np.reshape(final_image, (1, 28, 28, 1))\n",
        "\n",
        "    # Make prediction using the trained_model\n",
        "    prediction = words[np.argmax(trained_model.predict(final_image))]\n",
        "\n",
        "    return(prediction)"
      ],
      "metadata": {
        "id": "unIo_cEE8hvg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_expected_character_from_image_file_name(in_image_file_name):\n",
        "# Determine image character from file name A = img011-xxx.png Z = img036-xxx.png : Formula to determine ASCII value : (int(image_name[4:6]) - 11) + 65 : Convert ASCII value to character : chr(ascii_value)\n",
        "    \n",
        "    return_image_character = \"\"\n",
        "    image_reference_integer = int(in_image_file_name[4:6])\n",
        "    image_character_ASCII = (image_reference_integer-11) + 65\n",
        "    return_image_character = chr(image_character_ASCII)\n",
        "\n",
        "    return(return_image_character)"
      ],
      "metadata": {
        "id": "pyfexaZd_Zmg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_image_to_base64(in_image_file_name):\n",
        "# Function that takes in image file name reads in mage and convert to a base 64 string\n",
        "  file_name = (in_image_file_name)\n",
        "  with open(file_name, \"rb\") as img_file:\n",
        "      b64_string = base64.b64encode(img_file.read())\n",
        "\n",
        "  return(b64_string.decode('utf-8'))"
      ],
      "metadata": {
        "id": "7ycHiw-o8kDn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_from_base_64_to_image(in_base_64_image_string):\n",
        "# Function that takes in base 64 string and converts to image\n",
        "  image_webP = Image.open((io.BytesIO(base64.b64decode(in_base_64_image_string))))\n",
        "\n",
        "  image_RGB = image_webP.convert(\"RGB\")\n",
        "  \n",
        "  image = np.float32(image_RGB)\n",
        "\n",
        "  return(image)"
      ],
      "metadata": {
        "id": "c2BdjRR78nGd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_prediction_success_or_fail(in_expected_character,in_predicted_character):\n",
        "\n",
        "  if in_expected_character ==  in_predicted_character:\n",
        "    prediction_match = True\n",
        "  else:\n",
        "    prediction_match = False\n",
        "\n",
        "  return(prediction_match)"
      ],
      "metadata": {
        "id": "mu9e2pyOXgfA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create log reference\n",
        "validation_run_reference = dt.datetime.now(dt.timezone.utc)\n",
        "\n",
        "# Read images to be used for validation run\n",
        "column_names = ['run_date_time','image_reference','image_category','image_character','predicted_character','prediction_result','model_reference']\n",
        "prediction_results_df = pd.DataFrame(columns=column_names)\n",
        "prediction_results_df\n",
        "\n",
        "# Read images to be used for validation run\n",
        "directory_path = '/content/drive/MyDrive/Resized_images/'\n",
        "directory_listing = os.listdir(directory_path)\n",
        "\n",
        "for image_name in directory_listing:\n",
        "\n",
        "    image_file_name = (directory_path + image_name)\n",
        "\n",
        "    # Determine expected character\n",
        "    expected_character = determine_expected_character_from_image_file_name(image_name)\n",
        "    \n",
        "    # COnvert image to base 64 string\n",
        "    base_64_image_string = convert_image_to_base64(image_file_name)\n",
        "\n",
        "    # Return an image from the base64 string \n",
        "    image_returned = convert_from_base_64_to_image(base_64_image_string)\n",
        "\n",
        "    # Call letter prediction\n",
        "       \n",
        "    predicted_letter = predict_letter_from_word(image_returned)\n",
        "    prediction_result = determine_prediction_success_or_fail(expected_character,predicted_letter)\n",
        "\n",
        "    # Create a dataframe with a single entry of the results\n",
        "    result_list = [validation_run_reference,image_name,\"\",expected_character,predicted_letter,prediction_result,mod_data_input_file]\n",
        "    result_data_frame_entry = pd.DataFrame([result_list],columns=column_names)\n",
        "    result_data_frame_entry\n",
        "\n",
        "    # Output word built from prediction\n",
        "    # print(image_name , expected_character,predicted_letter)\n",
        "    prediction_results_df = prediction_results_df.append(result_data_frame_entry,ignore_index=True)\n"
      ],
      "metadata": {
        "id": "svBQehwq8yc3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataframe to sparks dataframe\n",
        "\n",
        "df_spark = spark.createDataFrame(prediction_results_df)"
      ],
      "metadata": {
        "id": "qgQmv3G35VVR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure settings for RDS\n",
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://finalpostgresdb.cxwdymdhaxq6.us-east-1.rds.amazonaws.com:5432/my_final_project\"\n",
        "config = {\"user\":\"root\", \"password\": \"Open4039!\", \"driver\":\"org.postgresql.Driver\"}\n",
        "\n",
        "df_spark.write.jdbc(url=jdbc_url, table=\"final_model_validation_run_logs\", mode=mode, properties=config)"
      ],
      "metadata": {
        "id": "Fhf8ed225nwX"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}